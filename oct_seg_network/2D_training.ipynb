{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "from helpers import *\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import monai\n",
    "import numpy as np\n",
    "import torch\n",
    "from monai.networks.utils import one_hot\n",
    "from monai.transforms import *\n",
    "from monai.losses import DiceFocalLoss, GeneralizedDiceFocalLoss\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_nsre = re.compile(\"([0-9]+)\")\n",
    "\n",
    "\n",
    "def natural_sort_key(s):\n",
    "    return [\n",
    "        int(text) if text.isdigit() else text.lower() for text in re.split(_nsre, s)\n",
    "    ]\n",
    "\n",
    "\n",
    "data_path = \"data/320063_intact-eye\"\n",
    "\n",
    "dataset_paths = [f.path for f in os.scandir(data_path) if f.is_dir()]\n",
    "\n",
    "images = []\n",
    "masks = []\n",
    "\n",
    "for dataset_path in dataset_paths:\n",
    "    img_file_names = sorted(\n",
    "        os.listdir(os.path.join(dataset_path, \"img\")), key=natural_sort_key\n",
    "    )\n",
    "    img_paths = [\n",
    "        os.path.join(dataset_path, \"img\", img_file_name)\n",
    "        for img_file_name in img_file_names\n",
    "    ]\n",
    "    mask_paths = [\n",
    "        os.path.join(dataset_path, \"masks_machine\", img_file_name[:-4] + \".png\")\n",
    "        for img_file_name in img_file_names\n",
    "    ]\n",
    "    images.extend(img_paths)\n",
    "    masks.extend(mask_paths)\n",
    "\n",
    "data_dict = [{\"img\": img, \"seg\": mask} for img, mask in zip(images, masks)]\n",
    "\n",
    "\n",
    "# Supervisely downloads masks that are empty as well, so we need to remove them\n",
    "for data in data_dict:\n",
    "    seg = data[\"seg\"]\n",
    "    seg_img = cv2.imread(seg, cv2.IMREAD_GRAYSCALE)\n",
    "    if not np.any(seg_img):\n",
    "        data_dict.remove(data)\n",
    "\n",
    "print(len(data_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets(split, train_batch_size, val_batch_size, test_batch_size):\n",
    "    train_data_list, val_data_list, test_data_list = monai.data.utils.partition_dataset(\n",
    "        data_dict, ratios=split, shuffle=True, seed=240899\n",
    "    )\n",
    "\n",
    "    train_transforms = monai.transforms.Compose(\n",
    "        [\n",
    "            LoadImaged(keys=[\"img\", \"seg\"]),\n",
    "            EnsureChannelFirstd(keys=[\"img\", \"seg\"]),\n",
    "            BorderPadd(keys=[\"img\", \"seg\"], spatial_border=(12, 0)),\n",
    "            Rotate90d(keys=[\"img\", \"seg\"], spatial_axes=[1, 0]),\n",
    "            Flipd(keys=[\"img\", \"seg\"], spatial_axis=[1]),\n",
    "            RandFlipd(keys=[\"img\", \"seg\"], prob=0.5, spatial_axis=1),\n",
    "            RandRotated(\n",
    "                keys=[\"img\", \"seg\"],\n",
    "                range_x=0.525,\n",
    "                prob=0.8,\n",
    "                mode=(\"bilinear\", \"nearest\"),\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    val_transforms = monai.transforms.Compose(\n",
    "        [\n",
    "            LoadImaged(keys=[\"img\", \"seg\"]),\n",
    "            EnsureChannelFirstd(keys=[\"img\", \"seg\"]),\n",
    "            BorderPadd(keys=[\"img\", \"seg\"], spatial_border=(12, 0)),\n",
    "            Rotate90d(keys=[\"img\", \"seg\"], spatial_axes=[1, 0]),\n",
    "            Flipd(keys=[\"img\", \"seg\"], spatial_axis=[1]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    test_transforms = monai.transforms.Compose(\n",
    "        [\n",
    "            LoadImaged(keys=[\"img\", \"seg\"]),\n",
    "            EnsureChannelFirstd(keys=[\"img\", \"seg\"]),\n",
    "            BorderPadd(keys=[\"img\", \"seg\"], spatial_border=(12, 0)),\n",
    "            Rotate90d(keys=[\"img\", \"seg\"], spatial_axes=[1, 0]),\n",
    "            Flipd(keys=[\"img\", \"seg\"], spatial_axis=[1]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    train_dataset = monai.data.Dataset(data=train_data_list, transform=train_transforms)\n",
    "    val_dataset = monai.data.Dataset(data=val_data_list, transform=val_transforms)\n",
    "    test_dataset = monai.data.Dataset(data=test_data_list, transform=test_transforms)\n",
    "\n",
    "    train_loader = monai.data.DataLoader(\n",
    "        train_dataset, batch_size=train_batch_size, num_workers=8\n",
    "    )\n",
    "    val_loader = monai.data.DataLoader(\n",
    "        val_dataset, batch_size=val_batch_size, num_workers=8\n",
    "    )\n",
    "    test_loader = monai.data.DataLoader(\n",
    "        test_dataset, batch_size=test_batch_size, num_workers=8\n",
    "    )\n",
    "\n",
    "    print(f\"Total dataset size: {len(data_dict)}\")\n",
    "    print(\n",
    "        f\"Num. train images: {len(train_dataset)}\\nNum. val images: {len(val_dataset)}\\nNum. test images: {len(test_dataset)}\"\n",
    "    )\n",
    "\n",
    "    # sanity check for shapes\n",
    "    print(f'Train image size: {monai.utils.first(train_loader)[\"img\"].shape}')\n",
    "    print(f'Train ground truth size: {monai.utils.first(train_loader)[\"seg\"].shape}')\n",
    "    print(\n",
    "        f'Class labels in ground truth: {np.unique(monai.utils.first(train_loader)[\"seg\"])}'\n",
    "    )\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_loss_optimizer_scheduler(\n",
    "    device,\n",
    "    lr,\n",
    "    num_channels,\n",
    "    num_res_units,\n",
    "    scheduler_factor,\n",
    "    scheduler_patience,\n",
    "    scheduler_min_lr,\n",
    "):\n",
    "    model = monai.networks.nets.UNet(\n",
    "        spatial_dims=2,\n",
    "        in_channels=1,\n",
    "        out_channels=4,\n",
    "        channels=tuple([2**x for x in range(num_channels[0], num_channels[1])]),\n",
    "        strides=tuple([2] * (num_channels[1] - num_channels[0] - 1)),\n",
    "        kernel_size=3,\n",
    "        num_res_units=num_res_units,\n",
    "    ).to(device)\n",
    "\n",
    "    loss = monai.losses.DiceLoss(include_background=True, softmax=True)\n",
    "    # loss = DiceFocalLoss(include_background=True, softmax=True, gamma=1, weight=torch.tensor([0.0043, 0.9990, 0.9982, 0.9983]))\n",
    "    # loss = GeneralizedDiceFocalLoss(include_background=True, softmax=True, gamma=0.6, weight=torch.tensor([0.005, 0.33, 0.33, 0.33]), lambda_gdl=0.5, lambda_focal=0.5)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode=\"min\",\n",
    "        factor=scheduler_factor,\n",
    "        patience=scheduler_patience,\n",
    "        min_lr=scheduler_min_lr,\n",
    "    )\n",
    "    return model, loss, optimizer, lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    device,\n",
    "    val_interval,\n",
    "    max_epochs,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    model,\n",
    "    optimizer,\n",
    "    loss_function,\n",
    "    lr_scheduler,\n",
    "    hparams_dict,\n",
    "    run_name=time.strftime(\"%Y%m%d-%H%M%S\"),\n",
    "):\n",
    "\n",
    "    run_path = os.path.join(\"runs\", run_name)\n",
    "    os.makedirs(run_path, exist_ok=False)\n",
    "\n",
    "    with open(os.path.join(run_path, \"hparams.pickle\"), \"wb\") as pickle_file:\n",
    "        pickle.dump(hparams_dict, pickle_file)\n",
    "\n",
    "    best_val_loss = np.inf\n",
    "    best_val_epoch = -1\n",
    "    epoch_loss_values = list()\n",
    "    val_loss_values = list()\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "\n",
    "        print(\"-\" * 10)\n",
    "        print(f\"epoch {epoch + 1}/{max_epochs}\")\n",
    "\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        step = 0\n",
    "\n",
    "        for batch_data in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            step += 1\n",
    "\n",
    "            inputs, labels = batch_data[\"img\"].to(device), batch_data[\"seg\"].to(device)\n",
    "            labels = one_hot(labels, num_classes=4)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_len = (\n",
    "                train_loader.batch_size * len(train_loader)\n",
    "            ) // train_loader.batch_size\n",
    "            print(f\"{step}/{epoch_len}, train_loss: {loss.item():.4f}\", end=\"\\r\")\n",
    "        epoch_loss /= step\n",
    "        epoch_loss_values.append(epoch_loss)\n",
    "        print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        if (epoch + 1) % val_interval == 0:\n",
    "\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            val_steps = 0\n",
    "            save_val_imgs = True\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for val_data in val_loader:\n",
    "                    val_steps += 1\n",
    "                    val_images, val_labels = val_data[\"img\"].to(device), val_data[\n",
    "                        \"seg\"\n",
    "                    ].to(device)\n",
    "                    val_labels = one_hot(val_labels, num_classes=4)\n",
    "\n",
    "                    val_output = model(val_images)\n",
    "                    val_loss += loss_function(val_output, val_labels).item()\n",
    "                    if save_val_imgs:\n",
    "                        val_output = torch.softmax(val_output, dim=1)\n",
    "                        save_val_results(val_labels.cpu().numpy(), val_output.cpu().numpy(), os.path.join(run_path, f\"val_results_{epoch+1}.png\"))\n",
    "                        save_val_imgs = False # we only save once per epoch\n",
    "                        \n",
    "\n",
    "                val_loss /= val_steps\n",
    "                val_loss_values.append(val_loss)\n",
    "                lr_scheduler.step(val_loss)\n",
    "                print(f\"epoch {epoch + 1} average validation loss: {val_loss:.4f}\")\n",
    "\n",
    "                # save best model\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    best_val_epoch = epoch + 1\n",
    "                    save_path = os.path.join(\n",
    "                        run_path, f\"best.pth\"\n",
    "                    )\n",
    "                    torch.save(model.state_dict(), save_path)\n",
    "                    print(\n",
    "                        f\"current epoch: {epoch + 1} current val loss: {val_loss:.4f} best val loss: {best_val_loss:.4f} at epoch {best_val_epoch}\"\n",
    "                    )\n",
    "                else:\n",
    "                    # early stopping\n",
    "                    if epoch - best_val_epoch > 20:\n",
    "                        print(\n",
    "                            f\"best val loss not updated for 20 epochs, stopping training\"\n",
    "                        )\n",
    "                        break\n",
    "\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            save_path = os.path.join(\n",
    "                run_path, f\"checkpoint_{epoch+1}_val_loss_{val_loss:.4f}.pth\"\n",
    "            )\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "\n",
    "    print(\n",
    "        f\"train completed, best val loss: {best_val_loss:.4f} at epoch: {best_val_epoch}\"\n",
    "    )\n",
    "    save_path = os.path.join(run_path, f\"final_{epoch+1}_val_loss{val_loss:.4f}.pth\")\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "\n",
    "    return epoch_loss_values, val_loss_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_loss_plots(epoch_loss_values, val_loss_values, run_name, val_interval):\n",
    "    # plot train and val losses\n",
    "    plt.figure(\"train\")\n",
    "    plt.title(\"Epoch Average Loss\")\n",
    "    x = [i + 1 for i in range(len(epoch_loss_values))]\n",
    "    x_val = [i for i in range(val_interval, len(x) + 1, val_interval)]\n",
    "    print(x_val)\n",
    "    train_loss = epoch_loss_values\n",
    "    val_loss = val_loss_values\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.plot(x, train_loss, color=\"red\")\n",
    "    plt.plot(x_val, val_loss, color=\"green\")\n",
    "    plt.legend([\"train\", \"val\"])\n",
    "    plt.savefig(f\"runs/{run_name}/loss_plot.png\")\n",
    "    plt.clf()\n",
    "    \n",
    "    losses = np.vstack([epoch_loss_values, val_loss_values]).T\n",
    "    np.savetxt(f\"runs/{run_name}/losses.csv\", losses, delimiter=\",\", fmt=\"%f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_and_save_results(device, model, test_loader, run_name, print_results=False):\n",
    "    times = []\n",
    "    class_wise_dice = []\n",
    "    class_wise_iou = []\n",
    "    class_wise_precision = []\n",
    "    class_wise_recall = []\n",
    "    class_wise_f1 = []\n",
    "    class_wise_accuracy = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for test_data in test_loader:\n",
    "            start_time = time.perf_counter()\n",
    "            test_images, test_labels = test_data[\"img\"].to(device), test_data[\"seg\"].to(\n",
    "                device\n",
    "            )\n",
    "\n",
    "            output = model(test_images)\n",
    "            predicted_mask = torch.argmax(torch.softmax(output, dim=1), dim=1)\n",
    "            elapsed_time = time.perf_counter() - start_time\n",
    "            # print(f\"elapsed time: {elapsed_time:.5f}\")\n",
    "\n",
    "            times.append(elapsed_time)\n",
    "\n",
    "            dice_scores = dice_coefficient_per_class(\n",
    "                predicted_mask, test_labels.squeeze(), 4\n",
    "            )\n",
    "            class_wise_dice.append(dice_scores)\n",
    "\n",
    "            iou_scores = iou_per_class(predicted_mask, test_labels.squeeze(), 4)\n",
    "            class_wise_iou.append(iou_scores)\n",
    "\n",
    "            precision_scores = precision_per_class(\n",
    "                predicted_mask, test_labels.squeeze(), 4\n",
    "            )\n",
    "            class_wise_precision.append(precision_scores)\n",
    "\n",
    "            recall_scores = recall_per_class(predicted_mask, test_labels.squeeze(), 4)\n",
    "            class_wise_recall.append(recall_scores)\n",
    "\n",
    "            f1_scores = f1_score_per_class(predicted_mask, test_labels.squeeze(), 4)\n",
    "            class_wise_f1.append(f1_scores)\n",
    "\n",
    "            accuracy_scores = accuracy_per_class(\n",
    "                predicted_mask, test_labels.squeeze(), 4\n",
    "            )\n",
    "            class_wise_accuracy.append(accuracy_scores)\n",
    "\n",
    "            # cv2.imshow('test', apply_color_map(predicted_mask[3].detach().cpu().numpy()))\n",
    "            # cv2.waitKey(0)\n",
    "            # cv2.destroyAllWindows()\n",
    "\n",
    "            # for i in range(test_data[\"img\"].shape[0]):\n",
    "            #     visualize_segmentation_results(\n",
    "            #         original_image=test_data[\"img\"][i][0].detach().cpu(),\n",
    "            #         ground_truth_mask=test_data[\"seg\"][i][0].detach().cpu(),\n",
    "            #         predicted_mask=predicted_mask[i].detach().cpu()\n",
    "            #     )\n",
    "\n",
    "    average_time = np.mean(times)\n",
    "    average_dice_scores = np.mean(class_wise_dice, axis=0)\n",
    "    average_iou_scores = np.mean(class_wise_iou, axis=0)\n",
    "    average_precision_scores = np.mean(class_wise_precision, axis=0)\n",
    "    average_recall_scores = np.mean(class_wise_recall, axis=0)\n",
    "    average_f1_scores = np.mean(class_wise_f1, axis=0)\n",
    "    average_accuracy_scores = np.mean(class_wise_accuracy, axis=0)\n",
    "\n",
    "    data = np.vstack(\n",
    "        [\n",
    "            average_dice_scores,\n",
    "            average_iou_scores,\n",
    "            average_precision_scores,\n",
    "            average_recall_scores,\n",
    "            average_f1_scores,\n",
    "            average_accuracy_scores,\n",
    "        ]\n",
    "    ).T\n",
    "\n",
    "    time_column = np.full((data.shape[0], 1), average_time)\n",
    "    data = np.hstack([data, time_column])\n",
    "\n",
    "    with open(f\"runs/{run_name}/results.csv\", \"w\") as f:\n",
    "        np.savetxt(\n",
    "            f\"runs/{run_name}/results.csv\",\n",
    "            data,\n",
    "            delimiter=\",\",\n",
    "            header=\"dice,iou,precision,recall,f1,accuracy,time\",\n",
    "            fmt=\"%f\",\n",
    "        )\n",
    "\n",
    "    if print_results:\n",
    "        print(f\"average elapsed time: {np.mean(times):.5f}\")\n",
    "        for cls in range(4):\n",
    "            print(f\"Average Dice Score for Class {cls}: {average_dice_scores[cls]:.5f}\")\n",
    "\n",
    "        for cls in range(4):\n",
    "            print(f\"Average IoU Score for Class {cls}: {average_iou_scores[cls]:.5f}\")\n",
    "\n",
    "        for cls in range(4):\n",
    "            print(\n",
    "                f\"Average Precision Score for Class {cls}: {average_precision_scores[cls]:.5f}\"\n",
    "            )\n",
    "\n",
    "        for cls in range(4):\n",
    "            print(\n",
    "                f\"Average Recall Score for Class {cls}: {average_recall_scores[cls]:.5f}\"\n",
    "            )\n",
    "\n",
    "        for cls in range(4):\n",
    "            print(f\"Average F1 Score for Class {cls}: {average_f1_scores[cls]:.5f}\")\n",
    "\n",
    "        for cls in range(4):\n",
    "            print(\n",
    "                f\"Average Accuracy Score for Class {cls}: {average_accuracy_scores[cls]:.5f}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    \"lr\": 1e-4,\n",
    "    \"epochs\": 1,\n",
    "    \"train_batch_size\": 16,\n",
    "    \"val_batch_size\": 16,\n",
    "    \"test_batch_size\": 16,\n",
    "    \"train_val_test_split\": [0.8, 0.1, 0.1],\n",
    "    \"val_interval\": 1,\n",
    "    \"num_channels\": [4, 9],\n",
    "    \"num_res_units\": 0,\n",
    "    \"scheduler_factor\": 0.1,\n",
    "    \"scheduler_patience\": 6,\n",
    "    \"scheduler_min_lr\": 1e-9,\n",
    "    \"run_name\": \"test_run\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_random_hparams():\n",
    "    lr = random.uniform(1e-6, 1e-2)\n",
    "    scheduler_min_lr = lr / 100000\n",
    "    batch_size = random.choice([4, 8, 16])\n",
    "    channel_start = random.randint(2, 4)\n",
    "    channel_len = random.randint(3, 6)\n",
    "    num_channels = [channel_start, channel_start + channel_len]\n",
    "    num_res_units = random.randint(0, 4)\n",
    "\n",
    "    hparams = {\n",
    "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "        \"lr\": lr,\n",
    "        \"epochs\": 250,\n",
    "        \"train_batch_size\": batch_size,\n",
    "        \"val_batch_size\": batch_size,\n",
    "        \"test_batch_size\": batch_size,\n",
    "        \"train_val_test_split\": [0.8, 0.1, 0.1],\n",
    "        \"val_interval\": 1,\n",
    "        \"num_channels\": num_channels,\n",
    "        \"num_res_units\": num_res_units,\n",
    "        \"scheduler_factor\": 0.1,\n",
    "        \"scheduler_patience\": 10,\n",
    "        \"scheduler_min_lr\": scheduler_min_lr,\n",
    "        \"run_name\": time.strftime(\"%Y%m%d-%H%M%S\"),\n",
    "    }\n",
    "    return hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_from_hparams(hparams, run_name=time.strftime(\"%Y%m%d-%H%M%S\")):\n",
    "    # hparams = create_random_hparams()\n",
    "    hparams[\"run_name\"] = run_name\n",
    "\n",
    "    print(\"-\" * 20)\n",
    "    print(hparams)\n",
    "\n",
    "    train_loader, val_loader, test_loader = create_datasets(\n",
    "        hparams[\"train_val_test_split\"],\n",
    "        hparams[\"train_batch_size\"],\n",
    "        hparams[\"val_batch_size\"],\n",
    "        hparams[\"test_batch_size\"],\n",
    "    )\n",
    "    model, loss_function, optimizer, lr_scheduler = (\n",
    "        create_model_loss_optimizer_scheduler(\n",
    "            hparams[\"device\"],\n",
    "            hparams[\"lr\"],\n",
    "            hparams[\"num_channels\"],\n",
    "            hparams[\"num_res_units\"],\n",
    "            hparams[\"scheduler_factor\"],\n",
    "            hparams[\"scheduler_patience\"],\n",
    "            hparams[\"scheduler_min_lr\"],\n",
    "        )\n",
    "    )\n",
    "\n",
    "    summary(model, input_size=(1, 1024, 1024))\n",
    "\n",
    "    epoch_loss_values, val_loss_values = train(\n",
    "        hparams[\"device\"],\n",
    "        hparams[\"val_interval\"],\n",
    "        hparams[\"epochs\"],\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        model,\n",
    "        optimizer,\n",
    "        loss_function,\n",
    "        lr_scheduler,\n",
    "        hparams,\n",
    "        hparams[\"run_name\"],\n",
    "    )\n",
    "\n",
    "    save_loss_plots(\n",
    "        epoch_loss_values, val_loss_values, hparams['run_name'], hparams[\"val_interval\"]\n",
    "    )\n",
    "\n",
    "    model.load_state_dict(torch.load(f\"runs/{hparams['run_name']}/best.pth\"))\n",
    "\n",
    "    test_and_save_results(\n",
    "        hparams[\"device\"], model, test_loader, hparams['run_name'], print_results=False\n",
    "    )\n",
    "\n",
    "    del model\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_from_hparams(hparams, run_name='test'+time.strftime(\"%Y%m%d-%H%M%S\"))\n",
    "\n",
    "# for i in range(15):\n",
    "#     print(f\"Training model {i+1}\")\n",
    "#     hparams = create_random_hparams()\n",
    "#     train_from_hparams(hparams)\n",
    "#     print(f\"Finished training {i+1} models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_run(run_path):\n",
    "    hparams_path = os.path.join(run_path, \"hparams.pickle\")\n",
    "    with open(hparams_path, \"rb\") as pickle_file:\n",
    "        hparams = pickle.load(pickle_file)\n",
    "\n",
    "    _, _, test_loader = create_datasets(hparams[\"train_val_test_split\"],\n",
    "        hparams[\"train_batch_size\"],\n",
    "        hparams[\"val_batch_size\"],\n",
    "        hparams[\"test_batch_size\"],)\n",
    "    \n",
    "    model, _, _, _ = create_model_loss_optimizer_scheduler(hparams['device'],\n",
    "                                                            hparams['lr'],\n",
    "                                                            hparams['num_channels'],\n",
    "                                                            hparams['num_res_units'],\n",
    "                                                            hparams['scheduler_factor'],\n",
    "                                                            hparams['scheduler_patience'],\n",
    "                                                            hparams['scheduler_min_lr'],)\n",
    "    \n",
    "    model.load_state_dict(\n",
    "        torch.load(\n",
    "            os.path.join(run_path, f\"best.pth\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    for image in test_loader:\n",
    "        img = image[\"img\"].to(hparams[\"device\"])\n",
    "        seg = image[\"seg\"].to(hparams[\"device\"])\n",
    "        output = model(img)\n",
    "        predicted_mask = torch.argmax(torch.softmax(output, dim=1), dim=1)\n",
    "        for i in range(img.shape[0]):\n",
    "            visualize_segmentation_results(\n",
    "                original_image=img[i][0].detach().cpu(),\n",
    "                ground_truth_mask=seg[i][0].detach().cpu(),\n",
    "                predicted_mask=predicted_mask[i].detach().cpu()\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intact eye fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, loss, optimizer, scheduler = create_model_loss_optimizer_scheduler(\"cuda\", 3e-5, [4, 10], 0, 0.1, 10, 1e-9)\n",
    "summary(model, input_size=(1, 1024, 1024))\n",
    "\n",
    "# model = monai.networks.nets.UNet(\n",
    "#                 spatial_dims=2,\n",
    "#                 in_channels=1,\n",
    "#                 out_channels=4,\n",
    "#                 channels=(16, 32, 64, 128, 256, 512),\n",
    "#                 strides=(2, 2, 2, 2, 2),\n",
    "#                 kernel_size=3,\n",
    "#             ).to(\"cuda\")\n",
    "\n",
    "# summary(model, input_size=(1, 1024, 1024))\n",
    "\n",
    "model.load_state_dict(torch.load(\"best_150_val_loss_0.4428_in_retina.pth\"))\n",
    "\n",
    "# freeze encoder layers\n",
    "# for idx, param in enumerate(model.parameters()):\n",
    "#     param.requires_grad = False\n",
    "#     if idx == 17:\n",
    "#         break\n",
    "\n",
    "train_loader, val_loader, test_loader = create_datasets([0.8, 0.1, 0.1], 8, 8, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = \"intact_retrain_large_lr_3\"\n",
    "val_interval = 1\n",
    "epoch_loss_values, val_loss_values = train(\"cuda\", val_interval=val_interval, max_epochs=100, train_loader=train_loader, val_loader=val_loader, model=model, optimizer=optimizer, loss_function=loss, lr_scheduler=scheduler, hparams_dict=None, run_name=run_name)\n",
    "save_loss_plots(epoch_loss_values, val_loss_values, run_name, val_interval)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "needle-seg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
